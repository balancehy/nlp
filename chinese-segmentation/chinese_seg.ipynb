{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import vstack, csr_matrix, coo_matrix\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, code=\"big5hkscs\"):\n",
    "    \"\"\"\n",
    "    Read bytes as list of string\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    num_errors = 0\n",
    "    for line in open(fpath, \"rb\"):\n",
    "        try:\n",
    "            lines.append(line.rstrip().decode(code))\n",
    "        except UnicodeDecodeError as e:\n",
    "            num_errors += 1\n",
    "    print('Encountered %d decoding errors.' % num_errors)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def extract_ngram(s, n, delimiter=' ', get_map=False):\n",
    "    \"\"\"\n",
    "    Extract n grams from string s, using defined single or multiple separator\n",
    "    \"\"\"\n",
    "    size = len(s)\n",
    "    if get_map: # result stored in set, no class label\n",
    "        res = set()\n",
    "    else: # result store in list, with class label\n",
    "        res = []\n",
    "    left = 0\n",
    "    while left<size:\n",
    "        if s[left] == delimiter:\n",
    "            left += 1\n",
    "        else:\n",
    "            idx = [left] # store indices for n grams\n",
    "            right = left\n",
    "            for i in range(1, n): # find n-1 non-separator characters\n",
    "                right += 1\n",
    "                while right<size: # find next non-separator characters\n",
    "                    if s[right]==delimiter:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        idx.append(right)\n",
    "                        break;\n",
    "            if len(idx) == n: # n grams found\n",
    "                a = int((len(idx)-1)/2) # lower median\n",
    "                temps = \"\"\n",
    "                for x in idx:\n",
    "                    temps += s[x]\n",
    "                    \n",
    "                if get_map:\n",
    "                    if temps not in res:\n",
    "                        res.add(temps)\n",
    "                else:\n",
    "                    if idx[a]+1 == idx[a+1]: # 4 gram \"abcd\", no seperator between b and c \n",
    "                        res.append([temps, 0])\n",
    "                    else:\n",
    "                        res.append([temps, 1])\n",
    "            left += 1\n",
    "                \n",
    "    return res\n",
    "\n",
    "def parse_all(data_list):\n",
    "    \"\"\"\n",
    "    Concatenate list of strings to one string, separated by double space \n",
    "    \"\"\"\n",
    "    temp_line = \"\"\n",
    "    for line in data_list:\n",
    "        temp_line += (line + \"  \")\n",
    "    \n",
    "    return temp_line\n",
    "\n",
    "def set2dict(s):\n",
    "    \"\"\"\n",
    "    Convert Set object to dict object\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for i, k in enumerate(s):\n",
    "        d[k] = i\n",
    "    return d\n",
    "\n",
    "def encode_word(gram4, grams_map):\n",
    "    \"\"\"\n",
    "    Using [ab, b, bc, c, cd] vector to encode 4-gram abcd. ab, b, bc, c, cd are encoded in a one vector whose\n",
    "    size is the number of unique 1 and 2 grams in training and testing datasets\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for g in gram4:\n",
    "        f, v = g # parse 4 gram and class label\n",
    "        sgram = np.array([f[0:2], f[1:2], f[1:3], f[2:3], f[2:4]]) # encoding vector\n",
    "        # Use coordinate format sparse matrix\n",
    "        row = np.zeros([len(sgram), ])\n",
    "        col = []\n",
    "        data = np.ones([len(sgram), ])\n",
    "        for e in sgram:\n",
    "            if e not in grams_map: # error handle when not find any element of encoding vector\n",
    "                raise KeyError(\"Feature not found.\")\n",
    "            col.append(grams_map[e]) # column number is the position in all 1 and 2 grams\n",
    "        a = coo_matrix((data, (row, col)), shape=[1, len(grams_map)])\n",
    "        x.append(a)\n",
    "        y.append(v)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    x = vstack(x)\n",
    "    y = np.stack(y, axis=0)\n",
    "    return x, y\n",
    "\n",
    "def evaluate(clf, x, y):\n",
    "    y_pred = clf.predict(x)\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y[i]==1:\n",
    "            tp += 1\n",
    "        if y_pred[i] == 1 and y[i] == 0:\n",
    "            fp += 1\n",
    "        if y_pred[i] == 0 and y[i] == 1:\n",
    "            fn += 1\n",
    "        if y_pred[i] == 0 and y[i] == 0:\n",
    "            tn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    acc = (tp + tn) / (tp + fp + tn + fn)\n",
    "    print(\"tp, fp, tn, fn: \", tp, fp, tn, fn)\n",
    "    \n",
    "    return precision, recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered 0 decoding errors.\n",
      "Encountered 11 decoding errors.\n"
     ]
    }
   ],
   "source": [
    "code = \"big5hkscs\"\n",
    "data_te_str = load_data(\"test.txt\", code=code)\n",
    "data_tr_str = load_data(\"training.txt\", code=code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_all(data_tr_str+data_te_str)\n",
    "data_tr = parse_all(data_tr_str)\n",
    "data_te = parse_all(data_te_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['時間  ：',\n",
       " '三月  十日  （  星期四  ）  上午  十時  。',\n",
       " '地點  ：',\n",
       " '學術  活動  中心  一樓  簡報室  。',\n",
       " '主講  ：',\n",
       " '民族所  所長  莊英章  先生  。',\n",
       " '講題  ：',\n",
       " '閩  、  台  漢人  社會  研究  的  若干  考察  。',\n",
       " '李  院長  於  二月  二十六日  至  三月  十五日  赴  美  訪問  ，',\n",
       " '期間  將  與  在  美  院士  商討  院務  ，']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr_str[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20631845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'時間  ：  三月  十日  （  星期四  ）  上午  十時  。  地點  ：  學術  活動  中心  一樓  簡報室  。  主講  ：  民族所  所長  莊英章  先生  。  講題  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data_tr[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get grams dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6140, 746397, 752537)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams1 = extract_ngram(data, 1, delimiter=' ', get_map=True)\n",
    "grams2 = extract_ngram(data, 2, delimiter=' ', get_map=True)\n",
    "ngrams = grams1.union(grams2)\n",
    "len(grams1), len(grams2), len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grams1, grams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = set2dict(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and testing data tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams4_tr = extract_ngram(data_tr, 4, delimiter=' ', get_map=False)\n",
    "grams4_te = extract_ngram(data_te, 4, delimiter=' ', get_map=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8976185, 18740)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grams4_tr), len(grams4_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gram_data_tr = encode_word(grams4_tr, ngrams)\n",
    "gram_data_te = encode_word(grams4_te, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier using logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = gram_data_tr\n",
    "x_tr = x_tr.tocsr() # row format of sparse matrix, for row slicing\n",
    "x_te, y_te = gram_data_te\n",
    "x_te = x_te.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, penalty=\"l2\", solver=\"sag\")\n",
    "clf.fit(x_tr[:, :], y_tr[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"logreg.mdpkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"logreg.mdpkl\", \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results.\n",
      "tp, fp, tn, fn:  11108 829 5929 874\n",
      "Precision:  0.9305520650079584\n",
      "Recall:  0.9270572525454849\n",
      "Accuracy:  0.9091248665955176\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing results.\")\n",
    "precision, recall, accuracy = evaluate(clf, x_te, y_te)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results.\n",
      "tp, fp, tn, fn:  5457796 289201 2880514 348674\n",
      "Precision:  0.9496778926454982\n",
      "Recall:  0.9399507790447552\n",
      "Accuracy:  0.9289369592984101\n"
     ]
    }
   ],
   "source": [
    "print(\"Training results.\")\n",
    "precision, recall, accuracy = evaluate(clf, x_tr, y_tr)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
