{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5R8pisbyi_MH"
   },
   "source": [
    "# Word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "n5l-IgMri_MJ",
    "outputId": "974e7f0e-1ae0-4638-bba2-8f6d88db7f50"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./src\") # the source code directory\n",
    "os.getcwd()\n",
    "from wordvector import WordVector\n",
    "from CBOW import CBOW\n",
    "import loaddoc\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juDPc3XOi_MM"
   },
   "source": [
    "# Load file text8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "cs2g8enQi_MO",
    "outputId": "84c82c26-3d9c-4032-9e3c-9181864b19b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 1 lines, 17005207 words.\n",
      "Dictionary size: 50000\n"
     ]
    }
   ],
   "source": [
    "files = [\"../data/text8\"] # the input file path\n",
    "word_array, dictionary, reverse_dict, num_lines, num_words = docload2.build_word_array(\n",
    "    files, vocab_size=50000, keep_punc=True, sent_token=False)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.\\nDictionary size: {}'\n",
    "      .format(num_lines, num_words, len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzNfmDK1dN_n"
   },
   "outputs": [],
   "source": [
    "# Limit the size of document to limit the size of vocabulary\n",
    "a = np.where(word_array[0:200000] == len(dictionary))[0]\n",
    "word_array[a] = np.random.randint(0, len(dictionary), len(a)) # replace non-existing words with random integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4zmC8Bpi_MS"
   },
   "source": [
    "## Neural Net Architecture\n",
    "![](notebook_images/NN_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hcphhSKEi_Mc",
    "outputId": "ace343e1-6b72-41a4-d2f3-9439fc95fe18",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "INFO:tensorflow:Summary name gradients/embed_layer/embedding_lookup_grad/Reshape:0 is illegal; using gradients/embed_layer/embedding_lookup_grad/Reshape_0 instead.\n",
      "INFO:tensorflow:Summary name gradients/hidden_layer/MatMul_grad/tuple/control_dependency_1:0 is illegal; using gradients/hidden_layer/MatMul_grad/tuple/control_dependency_1_0 instead.\n",
      "INFO:tensorflow:Summary name gradients/hidden_layer/add_grad/tuple/control_dependency_1:0 is illegal; using gradients/hidden_layer/add_grad/tuple/control_dependency_1_0 instead.\n",
      "INFO:tensorflow:Summary name gradients/nce_loss/embedding_lookup_grad/Reshape:0 is illegal; using gradients/nce_loss/embedding_lookup_grad/Reshape_0 instead.\n",
      "INFO:tensorflow:Summary name gradients/nce_loss/embedding_lookup_1_grad/Reshape:0 is illegal; using gradients/nce_loss/embedding_lookup_1_grad/Reshape_0 instead.\n",
      "Model built. Vocab size = 50000. Document length = 17005207 words.\n",
      "Training ...\n",
      "Epoch 1: total batch = 5624, loss_tr = 77.64, loss_va = 36.29, acc_va = 0.00\tSave better\n",
      "Epoch 2: total batch = 11248, loss_tr = 26.11, loss_va = 18.26, acc_va = 0.00\tSave better\n",
      "Epoch 3: total batch = 16872, loss_tr = 14.62, loss_va = 11.58, acc_va = 0.00\tSave better\n",
      "Epoch 4: total batch = 22496, loss_tr = 9.92, loss_va = 8.31, acc_va = 0.00\tSave better\n",
      "Epoch 5: total batch = 28120, loss_tr = 7.45, loss_va = 6.70, acc_va = 0.00\tSave better\n",
      "Epoch 6: total batch = 33744, loss_tr = 5.99, loss_va = 5.81, acc_va = 0.00\tSave better\n",
      "Epoch 7: total batch = 39368, loss_tr = 5.21, loss_va = 5.16, acc_va = 0.00\tSave better\n",
      "Epoch 8: total batch = 44992, loss_tr = 4.65, loss_va = 4.71, acc_va = 0.00\tSave better\n",
      "Epoch 9: total batch = 50616, loss_tr = 4.27, loss_va = 4.55, acc_va = 0.00\tSave better\n",
      "Epoch 10: total batch = 56240, loss_tr = 4.00, loss_va = 4.37, acc_va = 0.00\tSave better\n",
      "Epoch 11: total batch = 61864, loss_tr = 3.76, loss_va = 4.32, acc_va = 0.00\tSave better\n",
      "Epoch 12: total batch = 67488, loss_tr = 3.60, loss_va = 4.26, acc_va = 0.00\tSave better\n",
      "Epoch 13: total batch = 73112, loss_tr = 3.44, loss_va = 4.15, acc_va = 0.00\tSave better\n",
      "Epoch 14: total batch = 78736, loss_tr = 3.32, loss_va = 4.12, acc_va = 0.00\tSave better\n",
      "Epoch 15: total batch = 84360, loss_tr = 3.20, loss_va = 4.11, acc_va = 0.00\tSave better\n",
      "Epoch 16: total batch = 89984, loss_tr = 3.10, loss_va = 4.07, acc_va = 0.00\tSave better\n",
      "Epoch 17: total batch = 95608, loss_tr = 3.00, loss_va = 4.08, acc_va = 0.00\n",
      "\n",
      "Epoch 18: total batch = 101232, loss_tr = 2.91, loss_va = 4.09, acc_va = 0.00\n",
      "\n",
      "Epoch 19: total batch = 106856, loss_tr = 2.83, loss_va = 4.19, acc_va = 0.00\n",
      "\n",
      "Epoch 20: total batch = 112480, loss_tr = 2.75, loss_va = 4.14, acc_va = 0.00\n",
      "\n",
      "Epoch 21: total batch = 118104, loss_tr = 2.68, loss_va = 4.19, acc_va = 0.00\n",
      "\n",
      "Epoch 22: total batch = 123728, loss_tr = 2.61, loss_va = 4.25, acc_va = 0.00\n",
      "\n",
      "Epoch 23: total batch = 129352, loss_tr = 2.53, loss_va = 4.26, acc_va = 0.00\n",
      "\n",
      "Epoch 24: total batch = 134976, loss_tr = 2.47, loss_va = 4.34, acc_va = 0.00\n",
      "\n",
      "Epoch 25: total batch = 140600, loss_tr = 2.42, loss_va = 4.36, acc_va = 0.00\n",
      "\n",
      "Epoch 26: total batch = 146224, loss_tr = 2.35, loss_va = 4.34, acc_va = 0.00\n",
      "\n",
      "Epoch 27: total batch = 151848, loss_tr = 2.29, loss_va = 4.42, acc_va = 0.00\n",
      "\n",
      "Epoch 28: total batch = 157472, loss_tr = 2.23, loss_va = 4.43, acc_va = 0.00\n",
      "\n",
      "Epoch 29: total batch = 163096, loss_tr = 2.19, loss_va = 4.46, acc_va = 0.00\n",
      "\n",
      "Epoch 30: total batch = 168720, loss_tr = 2.13, loss_va = 4.55, acc_va = 0.00\n",
      "\n",
      "Epoch 31: total batch = 174344, loss_tr = 2.09, loss_va = 4.59, acc_va = 0.00\n",
      "\n",
      "Epoch 32: total batch = 179968, loss_tr = 2.04, loss_va = 4.63, acc_va = 0.00\n",
      "\n",
      "Epoch 33: total batch = 185592, loss_tr = 1.99, loss_va = 4.60, acc_va = 0.00\n",
      "\n",
      "Epoch 34: total batch = 191216, loss_tr = 1.95, loss_va = 4.66, acc_va = 0.00\n",
      "\n",
      "Epoch 35: total batch = 196840, loss_tr = 1.91, loss_va = 4.83, acc_va = 0.00\n",
      "\n",
      "Epoch 36: total batch = 202464, loss_tr = 1.86, loss_va = 4.83, acc_va = 0.00\n",
      "\n",
      "Epoch 37: total batch = 208088, loss_tr = 1.83, loss_va = 4.92, acc_va = 0.00\n",
      "\n",
      "Epoch 38: total batch = 213712, loss_tr = 1.79, loss_va = 4.94, acc_va = 0.00\n",
      "\n",
      "Epoch 39: total batch = 219336, loss_tr = 1.75, loss_va = 5.01, acc_va = 0.00\n",
      "\n",
      "Epoch 40: total batch = 224960, loss_tr = 1.72, loss_va = 5.06, acc_va = 0.00\n",
      "\n",
      "Epoch 41: total batch = 230584, loss_tr = 1.69, loss_va = 5.08, acc_va = 0.00\n",
      "\n",
      "Epoch 42: total batch = 236208, loss_tr = 1.66, loss_va = 5.12, acc_va = 0.00\n",
      "\n",
      "Epoch 43: total batch = 241832, loss_tr = 1.63, loss_va = 5.18, acc_va = 0.00\n",
      "\n",
      "Epoch 44: total batch = 247456, loss_tr = 1.61, loss_va = 5.27, acc_va = 0.00\n",
      "\n",
      "Epoch 45: total batch = 253080, loss_tr = 1.57, loss_va = 5.24, acc_va = 0.00\n",
      "\n",
      "Epoch 46: total batch = 258704, loss_tr = 1.55, loss_va = 5.39, acc_va = 0.00\n",
      "\n",
      "Epoch 47: total batch = 264328, loss_tr = 1.53, loss_va = 5.36, acc_va = 0.00\n",
      "\n",
      "Epoch 48: total batch = 269952, loss_tr = 1.50, loss_va = 5.40, acc_va = 0.00\n",
      "\n",
      "Epoch 49: total batch = 275576, loss_tr = 1.47, loss_va = 5.50, acc_va = 0.00\n",
      "\n",
      "Epoch 50: total batch = 281200, loss_tr = 1.45, loss_va = 5.56, acc_va = 0.00\n",
      "\n",
      "Epoch 51: total batch = 286824, loss_tr = 1.43, loss_va = 5.53, acc_va = 0.00\n",
      "\n",
      "Epoch 52: total batch = 292448, loss_tr = 1.41, loss_va = 5.60, acc_va = 0.00\n",
      "\n",
      "Epoch 53: total batch = 298072, loss_tr = 1.39, loss_va = 5.73, acc_va = 0.00\n",
      "\n",
      "Epoch 54: total batch = 303696, loss_tr = 1.38, loss_va = 5.74, acc_va = 0.00\n",
      "\n",
      "Epoch 55: total batch = 309320, loss_tr = 1.35, loss_va = 5.77, acc_va = 0.00\n",
      "\n",
      "Epoch 56: total batch = 314944, loss_tr = 1.33, loss_va = 5.84, acc_va = 0.00\n",
      "\n",
      "Epoch 57: total batch = 320568, loss_tr = 1.31, loss_va = 5.82, acc_va = 0.00\n",
      "\n",
      "Epoch 58: total batch = 326192, loss_tr = 1.30, loss_va = 5.87, acc_va = 0.00\n",
      "\n",
      "Epoch 59: total batch = 331816, loss_tr = 1.29, loss_va = 6.01, acc_va = 0.00\n",
      "\n",
      "Epoch 60: total batch = 337440, loss_tr = 1.27, loss_va = 6.01, acc_va = 0.00\n",
      "\n",
      "Epoch 61: total batch = 343064, loss_tr = 1.26, loss_va = 6.02, acc_va = 0.00\n",
      "\n",
      "Epoch 62: total batch = 348688, loss_tr = 1.25, loss_va = 6.11, acc_va = 0.00\n",
      "\n",
      "Epoch 63: total batch = 354312, loss_tr = 1.23, loss_va = 6.18, acc_va = 0.00\n",
      "\n",
      "Epoch 64: total batch = 359936, loss_tr = 1.22, loss_va = 6.16, acc_va = 0.00\n",
      "\n",
      "Epoch 65: total batch = 365560, loss_tr = 1.20, loss_va = 6.16, acc_va = 0.00\n",
      "\n",
      "Epoch 66: total batch = 371184, loss_tr = 1.19, loss_va = 6.20, acc_va = 0.00\n",
      "\n",
      "Epoch 67: total batch = 376808, loss_tr = 1.18, loss_va = 6.35, acc_va = 0.00\n",
      "\n",
      "Epoch 68: total batch = 382432, loss_tr = 1.17, loss_va = 6.32, acc_va = 0.00\n",
      "\n",
      "Epoch 69: total batch = 388056, loss_tr = 1.16, loss_va = 6.33, acc_va = 0.00\n",
      "\n",
      "Epoch 70: total batch = 393680, loss_tr = 1.15, loss_va = 6.41, acc_va = 0.00\n",
      "\n",
      "Epoch 71: total batch = 399304, loss_tr = 1.14, loss_va = 6.53, acc_va = 0.00\n",
      "\n",
      "Epoch 72: total batch = 404928, loss_tr = 1.13, loss_va = 6.60, acc_va = 0.00\n",
      "\n",
      "Epoch 73: total batch = 410552, loss_tr = 1.12, loss_va = 6.55, acc_va = 0.00\n",
      "\n",
      "Epoch 74: total batch = 416176, loss_tr = 1.11, loss_va = 6.62, acc_va = 0.00\n",
      "\n",
      "Epoch 75: total batch = 421800, loss_tr = 1.10, loss_va = 6.58, acc_va = 0.00\n",
      "\n",
      "Epoch 76: total batch = 427424, loss_tr = 1.09, loss_va = 6.65, acc_va = 0.00\n",
      "\n",
      "Epoch 77: total batch = 433048, loss_tr = 1.08, loss_va = 6.68, acc_va = 0.00\n",
      "\n",
      "Epoch 78: total batch = 438672, loss_tr = 1.07, loss_va = 6.61, acc_va = 0.00\n",
      "\n",
      "Epoch 79: total batch = 444296, loss_tr = 1.07, loss_va = 6.76, acc_va = 0.00\n",
      "\n",
      "Epoch 80: total batch = 449920, loss_tr = 1.06, loss_va = 6.82, acc_va = 0.00\n",
      "\n",
      "Epoch 81: total batch = 455544, loss_tr = 1.05, loss_va = 6.79, acc_va = 0.00\n",
      "\n",
      "Epoch 82: total batch = 461168, loss_tr = 1.04, loss_va = 6.78, acc_va = 0.00\n",
      "\n",
      "Epoch 83: total batch = 466792, loss_tr = 1.04, loss_va = 6.89, acc_va = 0.00\n",
      "\n",
      "Epoch 84: total batch = 472416, loss_tr = 1.03, loss_va = 7.03, acc_va = 0.00\n",
      "\n",
      "Epoch 85: total batch = 478040, loss_tr = 1.02, loss_va = 6.97, acc_va = 0.00\n",
      "\n",
      "Epoch 86: total batch = 483664, loss_tr = 1.02, loss_va = 7.00, acc_va = 0.00\n",
      "\n",
      "Epoch 87: total batch = 489288, loss_tr = 1.00, loss_va = 7.11, acc_va = 0.00\n",
      "\n",
      "Epoch 88: total batch = 494912, loss_tr = 1.01, loss_va = 7.11, acc_va = 0.00\n",
      "\n",
      "Epoch 89: total batch = 500536, loss_tr = 1.00, loss_va = 7.09, acc_va = 0.00\n",
      "\n",
      "Epoch 90: total batch = 506160, loss_tr = 0.99, loss_va = 7.08, acc_va = 0.00\n",
      "\n",
      "Epoch 91: total batch = 511784, loss_tr = 0.98, loss_va = 7.18, acc_va = 0.00\n",
      "\n",
      "Epoch 92: total batch = 517408, loss_tr = 0.98, loss_va = 7.18, acc_va = 0.00\n",
      "\n",
      "Epoch 93: total batch = 523032, loss_tr = 0.98, loss_va = 7.21, acc_va = 0.00\n",
      "\n",
      "Epoch 94: total batch = 528656, loss_tr = 0.97, loss_va = 7.23, acc_va = 0.00\n",
      "\n",
      "Epoch 95: total batch = 534280, loss_tr = 0.96, loss_va = 7.26, acc_va = 0.00\n",
      "\n",
      "Epoch 96: total batch = 539904, loss_tr = 0.95, loss_va = 7.29, acc_va = 0.00\n",
      "\n",
      "Epoch 97: total batch = 545528, loss_tr = 0.95, loss_va = 7.30, acc_va = 0.00\n",
      "\n",
      "Epoch 98: total batch = 551152, loss_tr = 0.95, loss_va = 7.33, acc_va = 0.00\n",
      "\n",
      "Epoch 99: total batch = 556776, loss_tr = 0.94, loss_va = 7.41, acc_va = 0.00\n",
      "\n",
      "Epoch 100: total batch = 562400, loss_tr = 0.94, loss_va = 7.39, acc_va = 0.00\n",
      "\n",
      "End Training: total batch = 562400, loss_tr = 0.94, loss_va = 7.40, acc_va = 0.00\n",
      "Save training history\n"
     ]
    }
   ],
   "source": [
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': len(dictionary),\n",
    "                \"seq_len\" : 4,\n",
    "                'embed_size': 100,\n",
    "                'hid_size': 100,\n",
    "                'neg_samples': 64,\n",
    "                'learning_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'drop_rate': 0.2,\n",
    "                'embed_noise': 0.1,\n",
    "                'hid_noise': 0.3,\n",
    "                'optimizer': 'Momentum',\n",
    "                'model_name': '../model-save/model_save'}\n",
    "\n",
    "print('Building training set ...')\n",
    "x, y = CBOW.build_training_set(word_array[0:200000], graph_params[\"seq_len\"])\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x_shuf[:split, :], y_shuf[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "\n",
    "model = CBOW(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, num_words))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=100, verbose=True, write_summary=True, \n",
    "                      pretrain_model=None)\n",
    "print(\"Save training history\")\n",
    "with open(\"../results/hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwAiJS7si_Me"
   },
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QATEnxd8hoWq"
   },
   "outputs": [],
   "source": [
    "with open(\"../results/hist.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FUwJpdbi_Mm"
   },
   "source": [
    "# Word Similarities\n",
    "Find closest words by calculating the cosine similarity between the given word and other words in dictionary. The cosine similarity uses two word embeding vectors.\n",
    "1. From the output of embedding layer\n",
    "2. From the output of hidden layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFN2N_5cKCBX"
   },
   "outputs": [],
   "source": [
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "JCWf2npni_Mn",
    "outputId": "9dbea15a-3138-41f8-c18b-bd6196b74f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'see'\n",
      "['however', 'but', 'produce', 'therefore', 'timor', 'employees', 'include', 'arsenal'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'see'\n",
      "['prevent', 'include', 'live', 'become', 'learn', 'script', 'call', 'show']\n"
     ]
    }
   ],
   "source": [
    "word = \"see\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7HA836Si_Mx"
   },
   "source": [
    "# Analogies  \n",
    "\n",
    "Find some candidates of D in the analogy that A to B is like C to D given A, B, C. It is done by calculating vector $\\mathbf{x_d}=\\mathbf{x_b}-\\mathbf{x_a}+\\mathbf{x_c}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RgomnRz_i_M0",
    "outputId": "243ec318-a831-4452-882f-1615dc40ee6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'has', 'is', 'contains', 'which']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('had', 'has', 'was', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3BqjKC8Xyre"
   },
   "source": [
    "# Predict Words in a Passage\n",
    "\n",
    "## Original Passage\n",
    "\n",
    "** american individualist anarchism benjamin tucker in one eight two five josiah warren had participated in a valiantly experiment headed by robert owen called new harmony which failed in a few years amidst much internal conflict warren blamed the community s failure on a lack of individual sovereignty and a lack of private property warren proceeded to organise crick anarchist communities which respected what he called the sovereignty of the individual at utopia and modern times in one eight three three warren wrote and published the peaceful anac which some have noted to be the first anarchist periodical ever published benjamin tucker says that warren**\n",
    "\n",
    "## Reconstructed Passage\n",
    "\n",
    "** american democratic anarchism benjamin tucker in one nine two five hours lincoln also thought in a seismic scale on by robert s unicameral new salem which failed in a few years because much internal conflict lincoln blamed the abacus s lives on a topic of individual anthropology and the number of private property aristotle proceeded to be grierson autistic although as held what he called the development of the individual at autistic and modern alchemy in one nine six three lincoln wrote and published a pacific disgraceful class they have said to be the first individual schools ever published benjamin rand says that rand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-ApP8-hki_M5",
    "outputId": "9e487657-527e-4e31-dbe6-e0f9e9ba0dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " american individualist anarchism benjamin tucker in one eight two five josiah warren had participated in a valiantly experiment headed by robert owen called new harmony which failed in a few years amidst much internal conflict warren blamed the community s failure on a lack of individual sovereignty and a lack of private property warren proceeded to organise crick anarchist communities which respected what he called the sovereignty of the individual at utopia and modern times in one eight three three warren wrote and published the peaceful anac which some have noted to be the first anarchist periodical ever published benjamin tucker says that warren\n"
     ]
    }
   ],
   "source": [
    "# grab 100 word passage from book\n",
    "# reverse_dict = word_vector_nce.get_reverse_dict()\n",
    "def get_passage(reverse_dict, word_array, start, end):\n",
    "    \n",
    "    passage = [x for x in map(lambda x: reverse_dict[x], word_array[start:end])]\n",
    "    # print passage with some crude formatting (e.g. space after comma)\n",
    "    readable = ''\n",
    "    for word in passage:\n",
    "        if word == '\"':\n",
    "            readable += word\n",
    "        elif word in ['?', '!', '.', ',']:\n",
    "            readable += word + ' '\n",
    "        else: \n",
    "            readable += ' ' + word\n",
    "    print(readable)\n",
    "\n",
    "get_passage(reverse_dict, word_array, 1000, 1104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "s3EZGZO7i_M8",
    "outputId": "a011c557-8b24-4f34-ec6d-2cd5f9a35c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model-save/model_save-5\n",
      " american democratic anarchism benjamin tucker in one nine two five hours lincoln also thought in a seismic scale on by robert s unicameral new salem which failed in a few years because much internal conflict lincoln blamed the abacus s lives on a topic of individual anthropology and the number of private property aristotle proceeded to be grierson autistic although as held what he called the development of the individual at autistic and modern alchemy in one nine six three lincoln wrote and published a pacific disgraceful class they have said to be the first individual schools ever published benjamin rand says that rand\n"
     ]
    }
   ],
   "source": [
    "def pred_passage(model, word_array, start, end, seq_len):\n",
    "    # use model to replace words in original passage with predicted words\n",
    "    # need to grab 2 words before and after passage\n",
    "    shift = seq_len // 2\n",
    "    x, y = CBOW.build_training_set(word_array[start-shift : end+shift], seq_len)\n",
    "    # x, y = CBOW.build_training_set(word_array[start-shift : end+shift])\n",
    "    y_pred = model.predict(x, \"5\")\n",
    "    passage_predict = [x for x in map(lambda x: reverse_dict[x], y_pred)]\n",
    "    # print predicted passage\n",
    "    readable = ''\n",
    "    for word in passage_predict:\n",
    "        if word == '\"':\n",
    "            readable += word\n",
    "        elif word in ['?', '!', '.', ',']:\n",
    "            readable += word + ' '\n",
    "        else: \n",
    "            readable += ' ' + word\n",
    "    print(readable)\n",
    "# model = CBOW(graph_params)\n",
    "pred_passage(model, word_array, 1000, 1104, graph_params[\"seq_len\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sherlock.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
